{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Keras와 Tensorflow를 결합한 딥러닝 모델 구현하기\n\n이번 세션에서는 Fashion MNIST 이미지 데이터를 활용하여 Keras의 Convolution Neural Network(CNN) 모델을 Azure Machine Learning Service에서 구현해보도록 하겠습니다. Azure Machine Learning Service에서는 총 세 가지 방법으로 딥러닝 모델을 구현할 수 있습니다.\n\n    1. 일반적인 모델링\n    2. 일반 Estimator를 활용한 모델링\n    3. Open Framework Estimator를 활용한 모델링\n\n세 가지 방법 중 두번째, 세번째 방법은 ML Workspace에서 Experiment를 Run하는 방식으로 진행이 되며 Data Science Virtual Machine 또는 AML Compute 등과 같은 computing target 사용이 가능합니다. 이번 세션에서는 Local 자원으로 첫 번째 방법을 실행해보고 AML Compute (GPU Cluster)를 활용하여 두번째, 세번째 방법을 실행해보고 마지막으로 Horovod를 사용하여 분산학습 (distributed training)을 사용해볼 예정입니다."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure ML 설정 (Configuration)\n\n아래 코드는 ML workspace 연동과 computing resource 지정 등을 수행합니다."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nimport warnings\nimport azureml.core\nimport os\nfrom azureml.core import Workspace\nfrom azureml.core import Experiment\nfrom azureml.core.compute import AmlCompute, ComputeTarget\n\nwarnings.simplefilter(\"ignore\")\n\nsubscription_id = '94ff7c1e-50c0-4466-a33b-232a0ccff39d'\nresource_group = 'amlStudy1'\nworkspace_name = 'amlMNIST'\nworkspace_region = 'eastus'\n\nws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n\nws.write_config()\n\n# GPU VM\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"mnistGPU\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Found compute target! Just use ' + compute_name)\nelse:\n    print('Creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n    compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n\nprint('Computing resources attached!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 데이터셋 불러오기\n\nFashion MNIST 데이터는 6만장의 패션 아이템 관련 Train 이미지 (28 X 28 픽셀)와 1만장의 Test 이미지로 구성되어 있습니다. 해당 데이터를 받을 수 있는 방법은 다양하지만 이번 세션에서는 tensorflow-keras 라이브러리에서 불러오도록 하겠습니다. 아래와 같이 tensorflow와 keras를 불러오고 datasets에서 fashion_mnist를 지정해준 뒤 load_data 클래스를 활용할 경우 쉽게 데이터를 불러올 수 있습니다. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "저장된 데이터의 사이즈를 확인하기 위해선 .shape를 사용하면 dimension을 확인할 수 있습니다."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\nprint(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig = plt.figure()\nseed = [12, 123, 1234, 12345, 1111, 2222, 3333, 4444, 5555, 9999]\nlabel = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nfor i in range(10):\n    subplot = fig.add_subplot(1, 10, i + 1)\n    subplot.set_xticks([])\n    subplot.set_yticks([])\n    subplot.set_title('%s' % label[y_train[seed[i]]])\n    subplot.imshow(x_train[seed[i]])\n\nfig.subplots_adjust(left=3, right=5, wspace=0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train = x_train.astype('float32')/255\nx_test = x_test.astype('float32')/255",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_x=x_train.reshape(60000,28,28,1)\ntest_x=x_test.reshape(10000,28,28,1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\n\ntrain_y = to_categorical(y_train)\ntest_y = to_categorical(y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"x_train shape:\", train_x.shape, \"y_train shape:\", train_y.shape)\nprint(\"x_test shape:\", test_x.shape, \"y_test shape:\", test_y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n\nmodel = Sequential()\n\n# First Convolution Layer\nmodel.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n\n# Second Convolution Layer\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n\n# Hidden Layer\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\n# Model Summary\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nstart0 = time.time()\n\nhistory=model.fit(train_x , \n                  train_y , \n                  batch_size=64 , \n                  epochs=10 ,\n                  validation_data = (test_x, test_y) ,\n                  shuffle=False)\n\nend0 = time.time()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "history.history['val_acc']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig2 = plt.figure()\npred_y = model.predict(test_x)\nseed2 = [12, 123, 1234, 12345, 1111, 2222, 3333, 4444, 5555, 7777, 9999]\n\nfor i in range(10):\n    subplot = fig2.add_subplot(1, 10, i + 1)\n    subplot.set_xticks([])\n    subplot.set_yticks([])\n    subplot.axis('off')\n    subplot.set_title('%s' % label[np.argmax(test_y[seed2[i]])])\n    subplot.imshow(x_test[seed2[i]])\n    subplot.text(0.5,-0.2, label[np.argmax(pred_y[seed2[i]])], size=12, ha=\"center\", transform=subplot.transAxes)\n\nfig2.subplots_adjust(left=3, right=5, wspace=0.1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "score = model.evaluate(test_x, test_y, verbose=0)\nrun0 = score[1]\nprint('\\n', 'Local Accuracy:', run0, '&', 'Processing Time:', end0-start0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training Script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = os.path.join(os.getcwd(), \"Training\")\nos.makedirs(script_folder, exist_ok=True)\nscript_folder",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train_cnn.py\n\nimport time\nimport argparse\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom azureml.core import Run\n\nstart = time.time()\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--batch_size', type=int, dest='batch_size')\nparser.add_argument('--epochs', type=int, dest='epochs')\nargs = parser.parse_args()\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nx_train = x_train.astype('float32')/255\nx_test = x_test.astype('float32')/255\n\ntrain_x=x_train.reshape(60000,28,28,1)\ntest_x=x_test.reshape(10000,28,28,1)\n\ntrain_y = to_categorical(y_train)\ntest_y = to_categorical(y_test)\n\nmodel = Sequential()\n\n# First Convolution Layer\nmodel.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n\n# Second Convolution Layer\nmodel.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n\n# Hidden Layer\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory=model.fit(train_x , \n                  train_y , \n                  batch_size=args.batch_size , \n                  epochs=args.epochs ,\n                  validation_data = (test_x, test_y) ,\n                  shuffle=False)\n\nend = time.time()\nt_process = end-start\n\nrun = Run.get_context()\nrun.log('processing_time', t_process)\nrun.log_list('accuracy', history.history['acc'])\nrun.log_list('loss', history.history['loss'])\nrun.log_list('val_accuracy', history.history['val_acc'])\nrun.log_list('val_loss', history.history['val_loss'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Estimator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--batch_size': 64 ,\n    '--epochs': 10 \n}\n\nest = Estimator(source_directory=script_folder ,\n                script_params=script_params ,\n                compute_target=compute_target ,\n                conda_packages=['tensorflow', 'keras'] ,\n                entry_script='train_cnn.py' ,\n                use_gpu=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'fashion-MNIST'\nexp1 = Experiment(workspace = ws, name = experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run1 = exp1.submit(est)\nrun1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run1).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run1.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run1_metrics = run1.get_metrics()\nprint('\\n', 'Estimator Accuracy:', run1_metrics['val_accuracy'][9], '&', 'Processing Time:', run1_metrics['processing_time'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## TensorFlow Estimator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import TensorFlow\n\nscript_params = {\n    '--batch_size': 64 ,\n    '--epochs': 10 \n}\n\nkeras_est = TensorFlow(source_directory=script_folder ,\n                       script_params=script_params ,\n                       compute_target=compute_target ,\n                       pip_packages=['keras'] ,\n                       entry_script='train_cnn.py' ,\n                       use_gpu=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'fashion-MNIST2'\nexp2 = Experiment(workspace = ws, name = experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run2 = exp2.submit(keras_est)\nrun2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.contrib.tensorboard import Tensorboard\n\n# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\ntb = Tensorboard([run])\n\n# If successful, start() returns a string with the URI of the instance.\ntb.start()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run2).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run2.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run2_metrics = run2.get_metrics()\nprint('\\n', 'TensorFlow Estimator Accuracy:', run2_metrics['val_accuracy'][9], '&', 'Processing Time:', run2_metrics['processing_time'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## TensorFlow with Horovod"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import TensorFlow\n\nscript_params = {\n    '--batch_size': 64 ,\n    '--epochs': 10 \n}\n\nkeras_est2 = TensorFlow(source_directory=script_folder ,\n                      script_params=script_params ,\n                      compute_target=compute_target ,\n                      node_count=2 ,\n                      process_count_per_node=1 ,\n                      pip_packages=['keras'] ,\n                      entry_script='train_cnn.py' ,\n                      distributed_backend='mpi' ,\n                      use_gpu=True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'fashion-MNIST3'\nexp3 = Experiment(workspace = ws, name = experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run3 = exp3.submit(keras_est2)\nrun3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run3).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run3.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run3_metrics = run3.get_metrics()\nprint('\\n', 'TensorFlow + Horovod Estimator Accuracy:',run3_metrics['val_accuracy'][9], '&', 'Processing Time:', run3_metrics['processing_time'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('\\n', 'Local Accuracy:', run0, '&', 'Processing Time:', end0-start0)\nprint('\\n', 'Estimator Accuracy:', run1_metrics['val_accuracy'][9], '&', 'Processing Time:', run1_metrics['processing_time'])\nprint('\\n', 'TensorFlow Estimator Accuracy:', run2_metrics['val_accuracy'][9], '&', 'Processing Time:', run2_metrics['processing_time'])\nprint('\\n', 'TensorFlow + Horovod Estimator Accuracy:', run3_metrics['val_accuracy'][9], '&', 'Processing Time:', run3_metrics['processing_time'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}